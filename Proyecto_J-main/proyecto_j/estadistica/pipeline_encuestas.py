"""
Pipeline Modular de Limpieza y Preprocesamiento de Datos de Encuesta
====================================================================

Este mÃ³dulo proporciona un conjunto completo de herramientas para el procesamiento
de datos de encuestas, incluyendo carga, exploraciÃ³n, limpieza, imputaciÃ³n,
anÃ¡lisis ponderado y exportaciÃ³n.
"""

import pandas as pd
import numpy as np
import streamlit as st
import missingno as msno
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from typing import Union, List, Dict, Any, Optional, Tuple
import os
import sys
import subprocess
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Imports para anÃ¡lisis estadÃ­stico
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor

# ImportaciÃ³n opcional de statsmodels
try:
    import statsmodels.api as sm
    from statsmodels.stats.weightstats import DescrStatsW
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("Warning: statsmodels no disponible. Algunas funciones de anÃ¡lisis ponderado pueden no funcionar.")

# ImportaciÃ³n del mÃ³dulo de validaciÃ³n de Chile
try:
    from .validacion_chile import (
        cargar_referencias, validar_geografia_chile, enriquecer_datos_chile,
        analisis_por_region, comparar_con_promedio_nacional,
        obtener_lista_regiones, obtener_lista_comunas, buscar_geografia_chile
    )
    VALIDACION_CHILE_AVAILABLE = True
except ImportError:
    try:
        from validacion_chile import (
            cargar_referencias, validar_geografia_chile, enriquecer_datos_chile,
            analisis_por_region, comparar_con_promedio_nacional,
            obtener_lista_regiones, obtener_lista_comunas, buscar_geografia_chile
        )
        VALIDACION_CHILE_AVAILABLE = True
    except ImportError:
        VALIDACION_CHILE_AVAILABLE = False
        print("Warning: MÃ³dulo de validaciÃ³n de Chile no disponible.")

# ============================================================================
# 1. CARGAR DATOS
# ============================================================================

@st.cache_data
def cargar_datos(ruta: str, validar_columnas: Optional[List[str]] = None) -> pd.DataFrame:
    """
    Carga datos desde mÃºltiples formatos con validaciÃ³n de columnas.
    
    Args:
        ruta: Ruta al archivo de datos
        validar_columnas: Lista de columnas que deben estar presentes
    
    Returns:
        DataFrame con los datos cargados
    
    Raises:
        FileNotFoundError: Si el archivo no existe
        ValueError: Si faltan columnas requeridas
    """
    if not os.path.exists(ruta):
        raise FileNotFoundError(f"El archivo no se encuentra en la ruta: {ruta}")

    filename = os.path.basename(ruta).lower()
    
    try:
        with st.spinner(f"Cargando datos desde {filename}..."):
            if filename.endswith(".sav"):
                import pyreadstat
                df, meta = pyreadstat.read_sav(ruta)
                st.success(f"Archivo SPSS cargado: {len(df)} filas, {len(df.columns)} columnas")
                
            elif filename.endswith(".dta"):
                import pyreadstat
                df, meta = pyreadstat.read_dta(ruta)
                st.success(f"Archivo Stata cargado: {len(df)} filas, {len(df.columns)} columnas")
                
            elif filename.endswith(".csv"):
                df = pd.read_csv(ruta)
                st.success(f"Archivo CSV cargado: {len(df)} filas, {len(df.columns)} columnas")
                
            elif filename.endswith((".xlsx", ".xls")):
                df = pd.read_excel(ruta)
                st.success(f"Archivo Excel cargado: {len(df)} filas, {len(df.columns)} columnas")
                
            else:
                raise ValueError("Formato de archivo no soportado. Use .sav, .dta, .csv, .xls, o .xlsx.")
        
        # Validar columnas requeridas
        if validar_columnas:
            columnas_faltantes = [col for col in validar_columnas if col not in df.columns]
            if columnas_faltantes:
                raise ValueError(f"Columnas requeridas faltantes: {columnas_faltantes}")
        
        return df
        
    except Exception as e:
        st.error(f"Error al cargar el archivo: {str(e)}")
        raise

# ============================================================================
# 1.5. PROCESAMIENTO DE ARCHIVOS GRANDES
# ============================================================================

def cargar_datos_grandes(ruta: str, chunk_size: int = 10000, 
                        validar_columnas: Optional[List[str]] = None) -> pd.DataFrame:
    """
    Carga archivos grandes en chunks para optimizar memoria.
    
    Args:
        ruta: Ruta al archivo de datos
        chunk_size: NÃºmero de filas por chunk
        validar_columnas: Lista de columnas que deben estar presentes
    
    Returns:
        DataFrame con los datos cargados
    """
    if not os.path.exists(ruta):
        raise FileNotFoundError(f"El archivo no se encuentra en la ruta: {ruta}")

    filename = os.path.basename(ruta).lower()
    
    try:
        with st.spinner(f"Cargando archivo grande desde {filename} en chunks..."):
            
            # Determinar el mÃ©todo de carga segÃºn el formato
            if filename.endswith(".csv"):
                # Para CSV, usar pandas read_csv con chunksize
                chunks = []
                total_rows = 0
                
                for chunk in pd.read_csv(ruta, chunksize=chunk_size):
                    chunks.append(chunk)
                    total_rows += len(chunk)
                    st.progress(min(total_rows / (total_rows + chunk_size), 0.95))
                
                df = pd.concat(chunks, ignore_index=True)
                
            elif filename.endswith((".xlsx", ".xls")):
                # Para Excel, cargar todo (no hay chunks nativos)
                df = pd.read_excel(ruta)
                
            elif filename.endswith(".sav"):
                # Para SPSS, cargar todo
                import pyreadstat
                df, meta = pyreadstat.read_sav(ruta)
                
            elif filename.endswith(".dta"):
                # Para Stata, cargar todo
                import pyreadstat
                df, meta = pyreadstat.read_dta(ruta)
                
            else:
                raise ValueError("Formato de archivo no soportado. Use .sav, .dta, .csv, .xls, o .xlsx.")
            
            st.success(f"Archivo cargado: {len(df):,} filas, {len(df.columns)} columnas")
            
            # Validar columnas requeridas
            if validar_columnas:
                columnas_faltantes = [col for col in validar_columnas if col not in df.columns]
                if columnas_faltantes:
                    raise ValueError(f"Columnas requeridas faltantes: {columnas_faltantes}")
            
            return df
            
    except Exception as e:
        st.error(f"Error al cargar el archivo: {str(e)}")
        raise

# ============================================================================
# 1.5. VALIDACIÃ“N Y ENRIQUECIMIENTO DE DATOS DE CHILE
# ============================================================================

def validar_datos_chile(df: pd.DataFrame, 
                       columna_region: Optional[str] = None,
                       columna_comuna: Optional[str] = None) -> Dict[str, Any]:
    """
    Valida los datos geogrÃ¡ficos de Chile en el DataFrame.
    
    Args:
        df: DataFrame a validar
        columna_region: Nombre de la columna con cÃ³digos de regiÃ³n
        columna_comuna: Nombre de la columna con cÃ³digos de comuna
    
    Returns:
        Diccionario con resultados de validaciÃ³n
    """
    if not VALIDACION_CHILE_AVAILABLE:
        st.warning("MÃ³dulo de validaciÃ³n de Chile no disponible")
        return {'valido': False, 'errores': ['MÃ³dulo de validaciÃ³n no disponible']}
    
    try:
        with st.spinner("Validando datos geogrÃ¡ficos de Chile..."):
            resultados = validar_geografia_chile(df, columna_region, columna_comuna)
            
            # Mostrar resultados
            if resultados['valido']:
                st.success("âœ… ValidaciÃ³n geogrÃ¡fica exitosa")
            else:
                st.error("âŒ Errores encontrados en validaciÃ³n geogrÃ¡fica")
                for error in resultados['errores']:
                    st.error(f"â€¢ {error}")
            
            if resultados['advertencias']:
                st.warning("âš ï¸ Advertencias encontradas:")
                for advertencia in resultados['advertencias']:
                    st.warning(f"â€¢ {advertencia}")
            
            return resultados
            
    except Exception as e:
        st.error(f"Error en validaciÃ³n de Chile: {str(e)}")
        return {'valido': False, 'errores': [str(e)]}

def enriquecer_datos_chile_pipeline(df: pd.DataFrame,
                                   columna_region: Optional[str] = None,
                                   columna_comuna: Optional[str] = None,
                                   incluir_indicadores: bool = True) -> pd.DataFrame:
    """
    Enriquece el DataFrame con datos oficiales de Chile.
    
    Args:
        df: DataFrame a enriquecer
        columna_region: Nombre de la columna con cÃ³digos de regiÃ³n
        columna_comuna: Nombre de la columna con cÃ³digos de comuna
        incluir_indicadores: Si incluir indicadores socioeconÃ³micos
    
    Returns:
        DataFrame enriquecido
    """
    if not VALIDACION_CHILE_AVAILABLE:
        st.warning("MÃ³dulo de validaciÃ³n de Chile no disponible")
        return df
    
    try:
        df_enriquecido = enriquecer_datos_chile(
            df, columna_region, columna_comuna, incluir_indicadores
        )
        
        # Mostrar informaciÃ³n del enriquecimiento
        columnas_nuevas = set(df_enriquecido.columns) - set(df.columns)
        if columnas_nuevas:
            st.success(f"âœ… Datos enriquecidos con {len(columnas_nuevas)} nuevas columnas")
            st.info(f"Columnas agregadas: {', '.join(sorted(columnas_nuevas))}")
        
        return df_enriquecido
        
    except Exception as e:
        st.error(f"Error al enriquecer datos: {str(e)}")
        return df

def analisis_regional_chile(df: pd.DataFrame,
                           variable_analizar: str,
                           columna_region: str = 'region_code') -> pd.DataFrame:
    """
    Realiza anÃ¡lisis estadÃ­stico por regiÃ³n de Chile.
    
    Args:
        df: DataFrame con datos
        variable_analizar: Variable a analizar
        columna_region: Columna con cÃ³digos de regiÃ³n
    
    Returns:
        DataFrame con estadÃ­sticas por regiÃ³n
    """
    if not VALIDACION_CHILE_AVAILABLE:
        st.warning("MÃ³dulo de validaciÃ³n de Chile no disponible")
        return pd.DataFrame()
    
    try:
        with st.spinner("Realizando anÃ¡lisis regional..."):
            analisis = analisis_por_region(df, variable_analizar, columna_region)
            
            # Mostrar resultados
            st.subheader("ğŸ“Š AnÃ¡lisis por RegiÃ³n")
            st.dataframe(analisis)
            
            # GrÃ¡fico de barras
            if 'region_nombre' in analisis.columns and 'mean' in analisis.columns:
                fig = px.bar(analisis, x='region_nombre', y='mean',
                            title=f"Promedio de {variable_analizar} por RegiÃ³n",
                            labels={'mean': 'Promedio', 'region_nombre': 'RegiÃ³n'})
                st.plotly_chart(fig, use_container_width=True)
            
            return analisis
            
    except Exception as e:
        st.error(f"Error en anÃ¡lisis regional: {str(e)}")
        return pd.DataFrame()

def comparar_nacional_chile(df: pd.DataFrame,
                           variable_analizar: str,
                           columna_region: str = 'region_code') -> pd.DataFrame:
    """
    Compara los valores por regiÃ³n con el promedio nacional.
    
    Args:
        df: DataFrame con datos
        variable_analizar: Variable a analizar
        columna_region: Columna con cÃ³digos de regiÃ³n
    
    Returns:
        DataFrame con comparaciones
    """
    if not VALIDACION_CHILE_AVAILABLE:
        st.warning("MÃ³dulo de validaciÃ³n de Chile no disponible")
        return pd.DataFrame()
    
    try:
        with st.spinner("Comparando con promedio nacional..."):
            comparacion = comparar_con_promedio_nacional(df, variable_analizar, columna_region)
            
            # Mostrar resultados
            st.subheader("ğŸ† ComparaciÃ³n con Promedio Nacional")
            st.dataframe(comparacion)
            
            # GrÃ¡fico de diferencias
            if 'region_nombre' in comparacion.columns and 'porcentaje_diferencia' in comparacion.columns:
                fig = px.bar(comparacion, x='region_nombre', y='porcentaje_diferencia',
                            title=f"Diferencia con Promedio Nacional - {variable_analizar}",
                            labels={'porcentaje_diferencia': 'Diferencia (%)', 'region_nombre': 'RegiÃ³n'},
                            color='categoria_diferencia')
                st.plotly_chart(fig, use_container_width=True)
            
            return comparacion
            
    except Exception as e:
        st.error(f"Error en comparaciÃ³n nacional: {str(e)}")
        return pd.DataFrame()

def mostrar_info_geografia_chile():
    """
    Muestra informaciÃ³n sobre la geografÃ­a de Chile.
    """
    if not VALIDACION_CHILE_AVAILABLE:
        st.warning("MÃ³dulo de validaciÃ³n de Chile no disponible")
        return
    
    try:
        st.subheader("ğŸ—ºï¸ InformaciÃ³n GeogrÃ¡fica de Chile")
        
        # InformaciÃ³n de regiones
        regiones = obtener_lista_regiones()
        st.write("**Regiones de Chile:**")
        st.dataframe(regiones)
        
        # EstadÃ­sticas bÃ¡sicas
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Total Regiones", len(regiones))
        with col2:
            comunas = obtener_lista_comunas()
            st.metric("Total Comunas", len(comunas))
        
        # BÃºsqueda de geografÃ­a
        st.write("**Buscar ubicaciÃ³n geogrÃ¡fica:**")
        termino = st.text_input("Ingrese nombre de regiÃ³n o comuna:")
        if termino:
            resultados = buscar_geografia_chile(termino)
            if not resultados.empty:
                st.dataframe(resultados)
            else:
                st.info("No se encontraron resultados")
                
    except Exception as e:
        st.error(f"Error al mostrar informaciÃ³n geogrÃ¡fica: {str(e)}")

# ============================================================================
# 2. EXPLORACIÃ“N DE DATOS
# ============================================================================

def explorar_datos(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Realiza exploraciÃ³n completa de los datos.
    
    Args:
        df: DataFrame a explorar
    
    Returns:
        Diccionario con estadÃ­sticas de exploraciÃ³n
    """
    with st.spinner("Explorando datos..."):
        info = {
            'shape': df.shape,
            'dtypes': df.dtypes.astype(str).value_counts().to_dict(),
            'memory_usage': df.memory_usage(deep=True).sum(),
            'null_counts': df.isnull().sum().to_dict(),
            'null_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),
            'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
            'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),
            'duplicate_rows': df.duplicated().sum()
        }
        
        # Mostrar informaciÃ³n bÃ¡sica
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Filas", info['shape'][0])
        with col2:
            st.metric("Columnas", info['shape'][1])
        with col3:
            st.metric("Filas duplicadas", info['duplicate_rows'])
        
        return info

def mostrar_resumen_datos(df: pd.DataFrame):
    """Muestra un resumen visual de los datos."""
    st.subheader("ğŸ“Š Resumen de Datos")
    
    # InformaciÃ³n bÃ¡sica
    info = explorar_datos(df)
    
    # Mostrar tipos de datos
    st.write("**Tipos de datos:**")
    tipo_counts = pd.Series(info['dtypes'])
    fig = px.bar(x=tipo_counts.index, y=tipo_counts.values, 
                 title="DistribuciÃ³n de Tipos de Datos")
    st.plotly_chart(fig, use_container_width=True)
    
    # Mostrar valores faltantes
    st.write("**Valores faltantes:**")
    null_df = pd.DataFrame({
        'columna': list(info['null_percentage'].keys()),
        'porcentaje': list(info['null_percentage'].values())
    }).sort_values('porcentaje', ascending=False)
    
    fig = px.bar(null_df, x='columna', y='porcentaje', 
                 title="Porcentaje de Valores Faltantes por Columna")
    st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# 3. VISUALIZACIÃ“N DE VALORES FALTANTES
# ============================================================================

def visualizar_nulos(df: pd.DataFrame, tipo: str = "matrix") -> None:
    """
    Visualiza valores faltantes usando missingno.
    
    Args:
        df: DataFrame a analizar
        tipo: Tipo de visualizaciÃ³n ('matrix', 'heatmap', 'dendrogram', 'bar')
    """
    with st.spinner("Generando visualizaciÃ³n de valores faltantes..."):
        st.subheader("ğŸ” AnÃ¡lisis de Valores Faltantes")
        
        # Crear figura y ejes
        fig, ax = plt.subplots(figsize=(12, 8))
        
        if tipo == "matrix":
            msno.matrix(df, ax=ax)
            
        elif tipo == "heatmap":
            msno.heatmap(df, ax=ax)
            
        elif tipo == "dendrogram":
            msno.dendrogram(df, ax=ax)
            
        elif tipo == "bar":
            msno.bar(df, ax=ax)
        
        # Ajustar layout y mostrar
        plt.tight_layout()
        st.pyplot(fig)
        plt.close(fig)  # Cerrar figura para liberar memoria
        
        # InformaciÃ³n adicional
        null_info = df.isnull().sum()
        st.write("**Resumen de valores faltantes:**")
        st.dataframe(null_info[null_info > 0].sort_values(ascending=False))

# ============================================================================
# 4. IMPUTACIÃ“N DE VALORES FALTANTES
# ============================================================================

@st.cache_data
def imputar_nulos(df: pd.DataFrame, estrategia: str = "knn", 
                  columnas_numericas: Optional[List[str]] = None,
                  columnas_categoricas: Optional[List[str]] = None) -> pd.DataFrame:
    """
    Imputa valores faltantes usando diferentes estrategias.
    
    Args:
        df: DataFrame a procesar
        estrategia: Estrategia de imputaciÃ³n ('media', 'mediana', 'moda', 'knn', 'random_forest')
        columnas_numericas: Columnas numÃ©ricas especÃ­ficas para imputar
        columnas_categoricas: Columnas categÃ³ricas especÃ­ficas para imputar
    
    Returns:
        DataFrame con valores imputados
    """
    df_limpio = df.copy()
    
    with st.spinner(f"Imputando valores faltantes usando estrategia: {estrategia}..."):
        
        # Determinar columnas a procesar
        if columnas_numericas is None:
            columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
        else:
            # Si se especificaron columnas numÃ©ricas explÃ­citamente, usar solo esas
            columnas_numericas = [col for col in columnas_numericas if col in df.columns]
            
        if columnas_categoricas is None:
            # Solo detectar automÃ¡ticamente columnas categÃ³ricas si NO se especificaron columnas numÃ©ricas especÃ­ficas
            # Esto permite control granular: si especificas columnas numÃ©ricas, las categÃ³ricas solo se procesan si se especifican explÃ­citamente
            if columnas_numericas is None or len(columnas_numericas) == len(df.select_dtypes(include=[np.number]).columns):
                columnas_categoricas = df.select_dtypes(include=['object', 'category']).columns.tolist()
            else:
                columnas_categoricas = []
        else:
            # Si se especificaron columnas categÃ³ricas explÃ­citamente, usar solo esas
            columnas_categoricas = [col for col in columnas_categoricas if col in df.columns]
        
        # Progreso
        progress_bar = st.progress(0)
        total_columns = len(columnas_numericas) + len(columnas_categoricas)
        current_progress = 0
        
        # ImputaciÃ³n para columnas numÃ©ricas
        if columnas_numericas and estrategia in ['media', 'mediana', 'knn', 'random_forest']:
            # Mapear estrategias en espaÃ±ol a scikit-learn
            estrategias_map = {'media': 'mean', 'mediana': 'median', 'moda': 'most_frequent'}
            estrategia_sklearn = estrategias_map.get(estrategia, estrategia)
            if estrategia in ['media', 'mediana']:
                imputer = SimpleImputer(strategy=estrategia_sklearn)
                df_limpio[columnas_numericas] = imputer.fit_transform(df_limpio[columnas_numericas])
            elif estrategia == 'knn':
                imputer = KNNImputer(n_neighbors=5)
                df_limpio[columnas_numericas] = imputer.fit_transform(df_limpio[columnas_numericas])
            elif estrategia == 'random_forest':
                for col in columnas_numericas:
                    if df_limpio[col].isnull().sum() > 0:
                        # Preparar datos para RF
                        temp_df = df_limpio.dropna(subset=[col])
                        X = temp_df[columnas_numericas].drop(columns=[col])
                        y = temp_df[col]
                        # Entrenar modelo
                        rf = RandomForestRegressor(n_estimators=100, random_state=42)
                        rf.fit(X, y)
                        # Predecir valores faltantes
                        mask = df_limpio[col].isnull()
                        if mask.sum() > 0:
                            X_pred = df_limpio.loc[mask, columnas_numericas].drop(columns=[col])
                            df_limpio.loc[mask, col] = rf.predict(X_pred)
            current_progress += len(columnas_numericas)
            progress_bar.progress(current_progress / total_columns)
        
        # ImputaciÃ³n para columnas categÃ³ricas
        if columnas_categoricas:
            estrategias_map = {'media': 'mean', 'mediana': 'median', 'moda': 'most_frequent'}
            estrategia_sklearn = estrategias_map.get(estrategia, estrategia)
            
            # Para estrategias que no son especÃ­ficamente para categÃ³ricas, usar moda por defecto
            if estrategia in ['media', 'mediana', 'knn', 'random_forest']:
                # Usar moda para columnas categÃ³ricas cuando la estrategia no es especÃ­fica
                imputer = SimpleImputer(strategy='most_frequent')
                df_limpio[columnas_categoricas] = imputer.fit_transform(df_limpio[columnas_categoricas])
            elif estrategia == 'moda':
                imputer = SimpleImputer(strategy=estrategia_sklearn)
                df_limpio[columnas_categoricas] = imputer.fit_transform(df_limpio[columnas_categoricas])
            
            current_progress += len(columnas_categoricas)
            progress_bar.progress(current_progress / total_columns)
        
        progress_bar.progress(1.0)
        st.success("ImputaciÃ³n completada exitosamente")
        
        return df_limpio

# ============================================================================
# 5. ANÃLISIS PONDERADO
# ============================================================================

def analisis_ponderado(df: pd.DataFrame, variable: str, peso: str) -> Dict[str, float]:
    """
    Realiza anÃ¡lisis estadÃ­stico ponderado.
    
    Args:
        df: DataFrame con los datos
        variable: Variable a analizar
        peso: Variable de peso
    
    Returns:
        Diccionario con estadÃ­sticas ponderadas
    """
    with st.spinner("Calculando estadÃ­sticas ponderadas..."):
        # Verificar que las columnas existen
        if variable not in df.columns:
            raise ValueError(f"Variable '{variable}' no encontrada en el DataFrame")
        if peso not in df.columns:
            raise ValueError(f"Variable de peso '{peso}' no encontrada en el DataFrame")
        
        # Filtrar datos vÃ¡lidos
        valid_data = df[[variable, peso]].dropna()
        
        if len(valid_data) == 0:
            raise ValueError("No hay datos vÃ¡lidos para el anÃ¡lisis ponderado")
        
        # Calcular estadÃ­sticas ponderadas
        if STATSMODELS_AVAILABLE:
            # Usar statsmodels si estÃ¡ disponible
            weighted_stats = DescrStatsW(valid_data[variable], weights=valid_data[peso])
            
            resultados = {
                'media_ponderada': weighted_stats.mean,
                'varianza_ponderada': weighted_stats.var,
                'desv_est_ponderada': np.sqrt(weighted_stats.var),
                'n_efectivo': weighted_stats.nobs,
                'suma_pesos': valid_data[peso].sum()
            }
        else:
            # ImplementaciÃ³n manual de estadÃ­sticas ponderadas
            x = valid_data[variable].values
            w = valid_data[peso].values
            
            # Media ponderada
            media_ponderada = np.average(x, weights=w)
            
            # Varianza ponderada
            varianza_ponderada = np.average((x - media_ponderada)**2, weights=w)
            
            # N efectivo
            n_efectivo = (w.sum()**2) / (w**2).sum()
            
            resultados = {
                'media_ponderada': media_ponderada,
                'varianza_ponderada': varianza_ponderada,
                'desv_est_ponderada': np.sqrt(varianza_ponderada),
                'n_efectivo': n_efectivo,
                'suma_pesos': w.sum()
            }
        
        # Mostrar resultados
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Media Ponderada", f"{resultados['media_ponderada']:.2f}")
        with col2:
            st.metric("Desv. Est. Ponderada", f"{resultados['desv_est_ponderada']:.2f}")
        with col3:
            st.metric("N Efectivo", f"{resultados['n_efectivo']:.0f}")
        
        return resultados

# ============================================================================
# 6. MUESTREO ESTRATIFICADO
# ============================================================================

def muestreo_estratificado(df: pd.DataFrame, columna_estratificacion: str, 
                          test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Realiza muestreo estratificado.
    
    Args:
        df: DataFrame a dividir
        columna_estratificacion: Columna para estratificaciÃ³n
        test_size: ProporciÃ³n del conjunto de prueba
        random_state: Semilla para reproducibilidad
    
    Returns:
        Tupla con (train_df, test_df)
    """
    with st.spinner("Realizando muestreo estratificado..."):
        if columna_estratificacion not in df.columns:
            raise ValueError(f"Columna de estratificaciÃ³n '{columna_estratificacion}' no encontrada")
        
        # Verificar que hay suficientes datos para estratificaciÃ³n
        if df[columna_estratificacion].nunique() > len(df) * test_size:
            st.warning("Demasiadas categorÃ­as para estratificaciÃ³n. Usando muestreo aleatorio simple.")
            train_idx, test_idx = train_test_split(
                df.index, test_size=test_size, random_state=random_state
            )
        else:
            train_idx, test_idx = train_test_split(
                df.index, test_size=test_size, 
                stratify=df[columna_estratificacion], 
                random_state=random_state
            )
        
        train_df = df.loc[train_idx].copy()
        test_df = df.loc[test_idx].copy()
        
        # Mostrar informaciÃ³n del muestreo
        st.success(f"Muestreo completado: {len(train_df)} train, {len(test_df)} test")
        
        # Visualizar distribuciÃ³n de estratos
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        train_df[columna_estratificacion].value_counts().plot(kind='bar', ax=ax1, title='Train')
        test_df[columna_estratificacion].value_counts().plot(kind='bar', ax=ax2, title='Test')
        
        plt.tight_layout()
        st.pyplot(fig)
        
        return train_df, test_df

# ============================================================================
# 7. EXPORTACIÃ“N Y REPORTES
# ============================================================================

def exportar_limpio(df: pd.DataFrame, ruta_salida: str, formato: str = "csv") -> str:
    """
    Exporta datos limpios en diferentes formatos.
    
    Args:
        df: DataFrame a exportar
        ruta_salida: Ruta de salida
        formato: Formato de exportaciÃ³n ('csv', 'excel', 'parquet')
    
    Returns:
        Ruta del archivo exportado
    """
    with st.spinner(f"Exportando datos en formato {formato}..."):
        try:
            if formato == "csv":
                df.to_csv(ruta_salida, index=False)
            elif formato == "excel":
                df.to_excel(ruta_salida, index=False)
            elif formato == "parquet":
                df.to_parquet(ruta_salida, index=False)
            else:
                raise ValueError(f"Formato no soportado: {formato}")
            
            st.success(f"Datos exportados exitosamente a: {ruta_salida}")
            return ruta_salida
            
        except Exception as e:
            st.error(f"Error al exportar datos: {str(e)}")
            raise

def generar_reporte_pdf(df: pd.DataFrame, output_path: str = "reporte_encuesta.pdf") -> str:
    """
    Genera un reporte PDF completo con anÃ¡lisis de datos.
    
    Args:
        df: DataFrame con los datos
        output_path: Ruta del archivo PDF de salida
    
    Returns:
        Ruta del archivo PDF generado
    """
    with st.spinner("Generando reporte PDF..."):
        # Crear archivo CSV temporal
        temp_csv_path = "temp_report_data.csv"
        df.to_csv(temp_csv_path, index=False)
        
        # Ruta al generador de reportes
        report_generator_path = os.path.join("tools", "csv-to-pdf-report-generator", "app.py")
        
        if not os.path.exists(report_generator_path):
            st.error(f"Generador de reportes no encontrado en: {report_generator_path}")
            return None
        
        try:
            # Ejecutar generador de reportes
            result = subprocess.run(
                [sys.executable, report_generator_path, temp_csv_path, output_path],
                capture_output=True,
                text=True,
                check=True
            )
            
            st.success(f"Reporte PDF generado exitosamente en: {output_path}")
            return output_path
            
        except subprocess.CalledProcessError as e:
            st.error(f"Error al generar reporte PDF: {e.stderr}")
            return None
            
        finally:
            # Limpiar archivo temporal
            if os.path.exists(temp_csv_path):
                os.remove(temp_csv_path)

# ============================================================================
# 8. PIPELINE PRINCIPAL
# ============================================================================

def pipeline_completo_encuestas(ruta_archivo: str, 
                               estrategia_imputacion: str = "knn",
                               generar_reporte: bool = True,
                               exportar_datos: bool = True) -> Dict[str, Any]:
    """
    Pipeline completo para procesamiento de datos de encuestas.
    
    Args:
        ruta_archivo: Ruta al archivo de datos
        estrategia_imputacion: Estrategia de imputaciÃ³n
        generar_reporte: Si generar reporte PDF
        exportar_datos: Si exportar datos limpios
    
    Returns:
        Diccionario con resultados del pipeline
    """
    resultados = {}
    
    try:
        # 1. Cargar datos
        st.header("ğŸ“ Carga de Datos")
        df = cargar_datos(ruta_archivo)
        resultados['datos_originales'] = df.copy()
        
        # 2. Explorar datos
        st.header("ğŸ” ExploraciÃ³n de Datos")
        mostrar_resumen_datos(df)
        
        # 3. Visualizar valores faltantes
        st.header("ğŸ” AnÃ¡lisis de Valores Faltantes")
        visualizar_nulos(df, tipo="matrix")
        
        # 4. Imputar valores faltantes
        st.header("ğŸ§¹ Limpieza de Datos")
        df_limpio = imputar_nulos(df, estrategia=estrategia_imputacion)
        resultados['datos_limpios'] = df_limpio
        
        # 5. Exportar datos limpios
        if exportar_datos:
            st.header("ğŸ’¾ ExportaciÃ³n de Datos")
            ruta_export = f"datos_limpios_{Path(ruta_archivo).stem}.csv"
            exportar_limpio(df_limpio, ruta_export)
            resultados['ruta_exportacion'] = ruta_export
        
        # 6. Generar reporte
        if generar_reporte:
            st.header("ğŸ“Š GeneraciÃ³n de Reporte")
            ruta_pdf = f"reporte_encuesta_{Path(ruta_archivo).stem}.pdf"
            pdf_path = generar_reporte_pdf(df_limpio, ruta_pdf)
            resultados['ruta_reporte'] = pdf_path
        
        st.success("ğŸ‰ Pipeline completado exitosamente!")
        return resultados
        
    except Exception as e:
        st.error(f"Error en el pipeline: {str(e)}")
        raise

# ============================================================================
# 9. FUNCIONES AUXILIARES
# ============================================================================

def validar_archivo(ruta: str) -> bool:
    """Valida que el archivo existe y es del formato correcto."""
    if not os.path.exists(ruta):
        return False
    
    extensiones_validas = ['.csv', '.sav', '.dta', '.xlsx', '.xls']
    return any(ruta.lower().endswith(ext) for ext in extensiones_validas)

def obtener_estadisticas_basicas(df: pd.DataFrame) -> Dict[str, Any]:
    """Obtiene estadÃ­sticas bÃ¡sicas del DataFrame."""
    return {
        'filas': len(df),
        'columnas': len(df.columns),
        'memoria_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'valores_faltantes': df.isnull().sum().sum(),
        'filas_duplicadas': df.duplicated().sum()
    }

def procesar_archivo_grande_en_chunks(ruta: str, chunk_size: int = 10000,
                                     estrategia_imputacion: str = "knn",
                                     generar_reporte: bool = True,
                                     exportar_datos: bool = True) -> Dict[str, Any]:
    """
    Procesa archivos grandes en chunks para optimizar memoria y rendimiento.
    
    Args:
        ruta: Ruta al archivo de datos
        chunk_size: TamaÃ±o del chunk para procesamiento
        estrategia_imputacion: Estrategia de imputaciÃ³n
        generar_reporte: Si generar reporte PDF
        exportar_datos: Si exportar datos limpios
    
    Returns:
        Diccionario con resultados del pipeline
    """
    resultados = {}
    
    try:
        # 1. Cargar datos en chunks
        st.header("ğŸ“ Carga de Datos (Archivo Grande)")
        df = cargar_datos_grandes(ruta, chunk_size)
        resultados['datos_originales'] = df.copy()
        
        # 2. ExploraciÃ³n bÃ¡sica (sin cargar todo en memoria)
        st.header("ğŸ” ExploraciÃ³n de Datos")
        info_basica = {
            'shape': df.shape,
            'dtypes': df.dtypes.astype(str).value_counts().to_dict(),
            'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024**2),
            'null_counts': df.isnull().sum().to_dict(),
            'null_percentage': (df.isnull().sum() / len(df) * 100).to_dict()
        }
        
        # Mostrar mÃ©tricas bÃ¡sicas
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Filas", f"{info_basica['shape'][0]:,}")
        with col2:
            st.metric("Columnas", info_basica['shape'][1])
        with col3:
            st.metric("Memoria (MB)", f"{info_basica['memory_usage_mb']:.1f}")
        with col4:
            st.metric("Valores faltantes", f"{df.isnull().sum().sum():,}")
        
        # 3. ImputaciÃ³n optimizada para archivos grandes
        st.header("ğŸ§¹ Limpieza de Datos")
        if info_basica['memory_usage_mb'] > 1000:  # MÃ¡s de 1GB
            st.warning("âš ï¸ Archivo muy grande detectado. Usando estrategia de imputaciÃ³n optimizada.")
            # Para archivos muy grandes, usar estrategias mÃ¡s simples
            if estrategia_imputacion in ['knn', 'random_forest']:
                estrategia_imputacion = 'media'  # Cambiar a estrategia mÃ¡s simple
                st.info(f"Estrategia cambiada a '{estrategia_imputacion}' para optimizar rendimiento")
        
        df_limpio = imputar_nulos(df, estrategia=estrategia_imputacion)
        resultados['datos_limpios'] = df_limpio
        
        # 4. Exportar datos limpios
        if exportar_datos:
            st.header("ğŸ’¾ ExportaciÃ³n de Datos")
            ruta_export = f"datos_limpios_{Path(ruta).stem}.csv"
            exportar_limpio(df_limpio, ruta_export)
            resultados['ruta_exportacion'] = ruta_export
        
        # 5. Generar reporte simplificado para archivos grandes
        if generar_reporte:
            st.header("ğŸ“Š GeneraciÃ³n de Reporte")
            ruta_pdf = f"reporte_encuesta_{Path(ruta).stem}.pdf"
            pdf_path = generar_reporte_pdf(df_limpio, ruta_pdf)
            resultados['ruta_reporte'] = pdf_path
        
        st.success("ğŸ‰ Pipeline para archivo grande completado exitosamente!")
        return resultados
        
    except Exception as e:
        st.error(f"Error en el pipeline para archivo grande: {str(e)}")
        raise

if __name__ == "__main__":
    st.title("ğŸ“Š Pipeline de Procesamiento de Datos de Encuestas")
    st.write("Este mÃ³dulo proporciona herramientas completas para el procesamiento de datos de encuestas.") 